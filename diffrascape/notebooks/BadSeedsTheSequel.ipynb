{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from diffrascape.env import BadSeedsTheSequel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_sequential_game(clist,vlist,bad_list,max_turns):\n",
    "    env = BadSeedsTheSequel(\n",
    "            centers=clist,\n",
    "            variances=vlist,\n",
    "            bad_seeds=bad_list,\n",
    "            max_turns=max_turns)\n",
    "\n",
    "    sum_points = 0\n",
    "    game_terminated = False\n",
    "\n",
    "    iguess = 0\n",
    "    while not game_terminated:\n",
    "        best_guess = int(iguess)\n",
    "        iguess += 1\n",
    "        next_state, game_terminated, next_reward = env.execute(iguess%env.N)\n",
    "        sum_points += next_reward\n",
    "    return sum_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential score 0.0\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "clist = N*[10.0]\n",
    "vlist = np.ones(N)*0.2\n",
    "vlist[0] *= 20.0 #20 times higher variance in bad ones\n",
    "bad_list = N*[False]\n",
    "bad_list[0] = 5*[True]\n",
    "max_turns = 20\n",
    "\n",
    "print (f'sequential score {play_sequential_game(clist, vlist, bad_list, max_turns)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jlynch/local/diffrascape/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "# $ tensorboard --logdir data/summaries\n",
    "# more state\n",
    "# more explore\n",
    "# expert trajectories\n",
    "\n",
    "from tensorforce.agents import Agent\n",
    "\n",
    "# N is the number of samples\n",
    "N = 30\n",
    "# n is the number of bad seeds\n",
    "n = 5\n",
    "clist = np.ones(N)*10.0\n",
    "vlist = np.ones(N)*0.2\n",
    "vlist[:n] = 0.2 * 20.0 #20 times higher variance in bad ones\n",
    "bad_list = N * [False]\n",
    "bad_list[:n] = n * [True]\n",
    "\n",
    "max_turns = 200\n",
    "\n",
    "bad_seeds_env = BadSeedsTheSequel(\n",
    "    centers=clist,\n",
    "    variances=vlist,\n",
    "    bad_seeds=bad_list,\n",
    "    max_turns=max_turns\n",
    ")\n",
    "\n",
    "good_ppo_agent = Agent.create(\n",
    "    #agent=\"tensorforce\",\n",
    "    #update=64,\n",
    "    #objective=\"policy_gradient\",\n",
    "    #reward_estimation=dict(horizon=max_turns),\n",
    "    \n",
    "    agent=\"ppo\",\n",
    "    batch_size=10,\n",
    "\n",
    "    #exploration=0.01,\n",
    "    variable_noise=0.01,\n",
    "    l2_regularization=0.1,\n",
    "    entropy_regularization=0.2,\n",
    "    \n",
    "    environment=bad_seeds_env,\n",
    "    max_episode_timesteps=max_turns,\n",
    "    summarizer=dict(\n",
    "        directory='data/summaries',\n",
    "        # list of labels, or 'all'\n",
    "        labels=['graph', 'entropy', 'kl-divergence', 'losses', 'rewards'],\n",
    "        frequency=100,  # store values every 100 timesteps\n",
    "    )\n",
    ")\n",
    "\n",
    "agent = Agent.create(\n",
    "    #agent=\"tensorforce\",\n",
    "    #update=64,\n",
    "    #objective=\"policy_gradient\",\n",
    "    #reward_estimation=dict(horizon=max_turns),\n",
    "    \n",
    "    agent=\"a2c\",\n",
    "    # ppo batch_size=10 works\n",
    "    # a2c batch_size=10 does not work but is this the problem?\n",
    "    batch_size=100, # this seems to help a2c\n",
    "\n",
    "    exploration=0.01,  # tried without this at first\n",
    "    variable_noise=0.05,\n",
    "    # variable_noise=0.01 bad?\n",
    "    l2_regularization=0.1,\n",
    "    entropy_regularization=0.2,\n",
    "    \n",
    "    # ppo: horizon=0 works\n",
    "    # a2c: horizon=0 works worse than ppo\n",
    "    horizon=200, # does this help a2c? yes\n",
    "\n",
    "    environment=bad_seeds_env,\n",
    "    max_episode_timesteps=max_turns,\n",
    "    summarizer=dict(\n",
    "        directory='data/summaries',\n",
    "        # list of labels, or 'all'\n",
    "        labels=['graph', 'entropy', 'kl-divergence', 'losses', 'rewards'],\n",
    "        frequency=100,  # store values every 100 timesteps\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes:  79%|███████▉  | 794/1000 [11:49, reward=0.00, ts/ep=200, sec/ep=0.95, ms/ts=4.8, agent=99.2%]  "
     ]
    }
   ],
   "source": [
    "from tensorforce.execution import Runner\n",
    "\n",
    "runner = Runner(agent=agent, environment=bad_seeds_env)\n",
    "for _ in range(10):\n",
    "    runner.run(num_episodes=1000)\n",
    "    agent.save(directory=\"saved_models\")\n",
    "#runner.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for n in (15, 10, 5):\n",
    "    clist = np.ones(N)*10.0\n",
    "    vlist = np.ones(N)*0.2\n",
    "    vlist[:n] = 0.2 * 20.0 #20 times higher variance in bad ones\n",
    "    bad_list = N * [False]\n",
    "    bad_list[:n] = n * [True]\n",
    "    #max_turns = 20\n",
    "\n",
    "    bad_seeds_env = BadSeeds(\n",
    "        clist=clist,\n",
    "        vlist=vlist,\n",
    "        bad_list=bad_list,\n",
    "        max_turns=max_turns\n",
    "    )\n",
    "\n",
    "    runner = Runner(agent=agent, environment=bad_seeds_env)\n",
    "    runner.run(num_episodes=5000)\n",
    "    #runner.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate for 100 episodes\n",
    "sum_rewards = 0.0\n",
    "for _ in range(100):\n",
    "    states = bad_seeds_env.reset()\n",
    "    print(states)\n",
    "    internals = agent.initial_internals()\n",
    "    terminal = False\n",
    "    while not terminal:\n",
    "        actions, internals = agent.act(states=states, internals=internals, evaluation=True)\n",
    "        states, terminal, reward = bad_seeds_env.execute(actions=actions)\n",
    "        sum_rewards += reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python36964bitvenvvenv2bd3108daf5d4eddb718eecc51a3e3e6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
